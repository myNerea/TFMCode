import requests
import json
import os
from utils.embedding import obtener_embedding_ollama
from utils.modelo_no_rag import responder_no_rag
from utils.rag_search import cargar_vectorstore, buscar_chunks_relevantes
from utils.chatdoctorado import ChatDoctorado
from utils.resumidor import ResumidorLlama
from utils.traduccion_pregunta import preparar_pregunta
from utils.traduccion_respuesta import traducir_respuesta
from utils.expandir_pregunta import expandir_pregunta
from utils.clarificador import generar_pregunta_clarificacion
from utils.social import modulo_social
from utils.modulo_reexplicacion import modulo_reexplicacion
from utils.prevencion import analizar_actitud_maliciosa, generar_respuesta_segura
from evaluacion.scripts.evaluador_traduccion import evaluar_traduccion
from evaluacion.scripts.evaluador_rag import evaluar_respuesta_con_llm, obtener_todos_los_chunks_relevantes
from evaluacion.scripts.evaluador_generativo import evaluar_saludo_social, evaluar_reexplicacion, evaluar_pregunta_clarificacion, evaluar_respuesta_no_rag
from evaluacion.scripts.latencia import Cronometro 




class AgenteDecisor:
    def __init__(self, umbral=0.5, top_k=3):
        self.vectorstore = cargar_vectorstore()
        self.resumidor = ResumidorLlama() # Creamos una instancia de la clase resumidor. 
        # Guardandola como atributo de la clase actual para poder llamarla posteriormente.
        self.historial_conversacion = [] # Aqu√≠ iremos guardando el historial de la conversaci√≥n.
        self.historial_preguntas_usuario = [] # Aqu√≠ vamos guardando las preguntas que va haciendo el usuario para tenerlas como contexto.
        self.historial_resumen = ""
        self.chat_rag = ChatDoctorado(self.historial_conversacion) # Creamos una instancia de la clase chatDoctorado 
        self.umbral = umbral
        self.top_k = top_k
        self.esperando_aclaracion = False
        self.ultima_pregunta_original = None

    # Funci√≥n para determinar si entrar o no en el RAG
    def es_pregunta_relevante_llm(self, pregunta, resumen_historial, modelo="llama3.2"):
        """
        Consulta al modelo para determinar si debe entrar en m√≥dulo de RAG.
        El modelo responde s√≠/no a si la pregunta o comentario plantea algo relacionado con el doctorado.
        """
        
        url_api = "http://localhost:11434/api/chat"

        prompt = (
            "Tu tarea es decidir si la siguiente pregunta est√° relacionada con temas de doctorado en la Universidad de Sevilla (US). "
            "Esto incluye requisitos, becas, procedimientos, plazos, admisiones, matr√≠cula, normativa, equivalencias, documentaci√≥n, etc. "
            "Se te proporciona un resumen de la conversaci√≥n anterior para que lo tengas en cuenta en tu respuesta.\n\n"
            f"Resumen del historial:\n{resumen_historial}\n\n"
            f"Pregunta actual: {pregunta}\n\n"
            "¬øEst√° esta pregunta relacionada con temas de doctorado en la US?" 
            "Responde solo con 's√≠' o 'no'."
        )

        data = {
            "model": modelo,
            "messages": [
                {
                    "role": "system",
                    "content": "Eres un asistente que determina si una pregunta esta relacionada con doctorados en la Universidad de Sevilla, usando como apoyo en el resumen de la conversaci√≥n."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": True
        }

        try:
            # Llamamos al modelo en la URL definida anteriormente y le pasamos los datos(el prompt completo)
            # Stream=True permite que se vaya leyendo tal cual va llegando por l√≠neas o por chunks
            response = requests.post(url_api, json=data, stream=True)
            if response.status_code == 200:
                # Si obtenemos una respuesta satisfactora, la leemos
                # Esta lectura se realiza l√≠nea a l√≠nea para evitar errores de formato JSON
                respuesta_completa = ""
                for line in response.iter_lines(decode_unicode=True):
                    # decode_unicode=True decodifica cada l√≠nea de bytes a string (UTF-8)
                    if line:
                        try:
                            json_data = json.loads(line)
                            # Cada l√≠nea se intenta interpretar como JSON.
                            content = json_data.get("message", {}).get("content", "")
                            # Dentro del mensaje se extrae el contenido para guardarlo
                            respuesta_completa += content
                        except json.JSONDecodeError:
                            pass
                respuesta = respuesta_completa.strip().lower()
                # Obtenmos las respuesta limpia(sin espacios delante o atr√°s y en minusculas)
                return respuesta.startswith("s√≠") or respuesta.startswith("si")
                # Devuelve True si la respuesta del modelo empieza por s√≠ o por si y devuelve 
                # False en caso contrario.
            else:
                print(f"‚ö†Ô∏è Error en API LLM: c√≥digo {response.status_code}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error al consultar LLM para relevancia con historial: {e}")

        return False
    
    # Funci√≥n para determinar si es un saludo, despedida, o agradecimiento puro
    # Nota: El funcionamiento es igual que el anterior.
    def es_saludo_social_llm(self, texto, modelo="llama3.2"):
        """
        Llama al LLM para detectar si el texto es UNICAMENTE es un saludo, despedida o agradecimiento.
        El modelo devuelve s√≠/no en funci√≥n de si lo es o no.
        Tras una comprobaci√≥n, la funci√≥n devuelve True si s√≠ es un saludo, despedida o agradecimiento
        y devuelve False si no lo es.
        """
        url_api = "http://localhost:11434/api/chat"

        prompt = f"""
Eres un asistente que detecta si el usuario est√° saludando, despidi√©ndose o dando las gracias.
Responde SOLO con "s√≠" si el texto es un saludo, despedida o agradecimiento completamente independiente (p.ej. solo "hola", "gracias", "adi√≥s").
Responde SOLO con "no" si el texto es m√°s complejo, contiene m√°s informaci√≥n o pregunta.

Texto: "{texto}"
¬øEs este texto √∫nicamente un saludo, despedida o agradecimiento? Responde solo con "s√≠" o "no".
""".strip()

        data = {
            "model": modelo,
            "messages": [
                {"role": "system", "content": "Eres un asistente que detecta saludos, despedidas o agradecimientos puros."},
                {"role": "user", "content": prompt}
            ],
            "stream": True
        }

        try:
            response = requests.post(url_api, json=data, stream=True)
            if response.status_code == 200:
                respuesta_completa = ""
                for line in response.iter_lines(decode_unicode=True):
                    if line:
                        try:
                            json_data = json.loads(line)
                            content = json_data.get("message", {}).get("content", "")
                            respuesta_completa += content
                        except json.JSONDecodeError:
                            pass
                respuesta = respuesta_completa.strip().lower()
                return respuesta.startswith("s√≠") or respuesta.startswith("si")
            else:
                print(f"‚ö†Ô∏è Error en API LLM: c√≥digo {response.status_code} en saludo social")
        except Exception as e:
            print(f"‚ö†Ô∏è Error al consultar LLM para saludo social: {e}")

        return False

    # Funci√≥n para determinar si es necesaria una reexplicaci√≥n
    # Nota: El funcionamiento es igual que el anterior
    def debe_reexplicar_llm(self, pregunta_usuario, modelo="llama3.2"):
        """
        Consulta al modelo para determinar si debe entrar en m√≥dulo de reexplicaci√≥n.
        El modelo responde s√≠/no a si la pregunta o comentario indica que no ha entendido la respuesta previa.
        """
        url_api = "http://localhost:11434/api/chat"
        prompt = (
            "Eres un asistente que debe decidir si el usuario no ha comprendido la respuesta previa y necesita que se le explique de forma m√°s clara.\n"
            "Devuelve SOLO 's√≠' o 'no'.\n\n"
            "Ejemplo:" 
            "Usuario: No lo he entendido -> s√≠" 
            "Usuario: No he comprendido muy bien lo que me has dicho -> s√≠"
            "Usuario: ¬øqu√© requisitos hay para acceder a un programa de doctorado? -> no"
            f"Usuario: {pregunta_usuario}\n\n"
            "¬øDebe el asistente intentar reexplicar la √∫ltima respuesta?"
        )
        data = {
            "model": modelo,
            "messages": [
                {"role": "system", "content": "Decide si la pregunta indica falta de comprensi√≥n y necesita reexplicaci√≥n."},
                {"role": "user", "content": prompt}
            ],
            "stream": True
        }

        try:
            response = requests.post(url_api, json=data, stream=True)
            if response.status_code == 200:
                respuesta_completa = ""
                for line in response.iter_lines(decode_unicode=True):
                    if line:
                        try:
                            json_data = json.loads(line)
                            content = json_data.get("message", {}).get("content", "")
                            respuesta_completa += content
                        except json.JSONDecodeError:
                            pass
                respuesta = respuesta_completa.strip().lower()
                return respuesta.startswith("s√≠") or respuesta.startswith("si")
            else:
                print(f"‚ö†Ô∏è Error en API LLM para decidir reexplicaci√≥n: c√≥digo {response.status_code}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error al consultar LLM para decidir reexplicaci√≥n: {e}")

        return False


    # Funci√≥n que genera la respuesta para el usuario
    def responder(self, pregunta_usuario):
        """
        Esta funci√≥n se encarga de generar la respuesta que recibir√° el usuario.
        """

        # Creamos una nueva instancia de la clase Cronometro definida en latencia.py
        # Al crear esta nueva instancia iniciamos el cronometro que nos determinar√° el tiempo que se tarda
        # en dar una respuesta al usuario.
        cronometro = Cronometro()
        # Guardamos a continuaci√≥n la pregunta del usuario
        cronometro.set_pregunta(pregunta_usuario)

        # Iniciamos el context manager para medir el tiempo que tarda en el m√≥dulo de traducci√≥n.
        with cronometro.medir("traduccion"):
            # Llamamos a la funci√≥n para preparar la pregunta que se encuentra en traduccion_pregunta.py
            pregunta_traducida, idioma_original, mezcla = preparar_pregunta(pregunta_usuario)

        # Salimos del contexto manager creado para traduccion al terminar la indentacion por lo que se guarda
        # el tiempo que hemos tardado en traducirlo.
        # Si ocurre un error o una excepci√≥n para tambi√©n el tiempo.

        # Definimos una variable que necesitaremos en el siguiente bloque with
        respuesta_final = None

        # Entramos en un nuevo bloque with. Esta vez mediremos el tiempo para el m√≥dulo de actitud maliciosa
        with cronometro.medir("analisis_actitud_maliciosa"):
            # Llamamos a la funci√≥n de prevencion.py para ver si el usuario tiene o no una actitud maliciosa
            # En caso de ser True la respuesta de la funci√≥n entra dentro del bloque if
            if analizar_actitud_maliciosa(pregunta_traducida):
                print("‚ö†Ô∏è Se detect√≥ intento de manipulaci√≥n, se genera respuesta segura.")
                # Entramos en otro with para medir el tiempo que tarda en generar la respuesta segura
                # T√©ngase en cuenta, que del primero no se sale hasta que no termine su bloque, el cual termina
                # cuando deja de estar indentado, llega a un returno, break o error.
                with cronometro.medir("generar_respuesta_segura"):
                    respuesta_segura = generar_respuesta_segura()

                    # Comprobamos si no era espa√±ol y no ten√≠a mezcla
                    if idioma_original != "espa√±ol":
                        # En este caso nos vamos a la funcion de traduccion_respuesta.py
                        respuesta_final = traducir_respuesta(respuesta_segura, idioma_original)
                    else:
                        respuesta_final = respuesta_segura

        # Guardamos el JSON generado. Esto para autom√°ticamente los dos with recolectando su tiempo
        cronometro.guardar_json()

        # Comprobamos si ha entrado en el bloque para generar una respuesta segura en cuyo caso devolvemos
        # el valor predefinido.
        # Tras hacer el return, se sale de la funci√≥n y no ejecuta nada m√°s.
        if respuesta_final is not None:
            return {
                "respuesta": respuesta_final,
                "necesita_aclaracion": self.esperando_aclaracion
            }

        # Definimos la variable que necesitaremos en los siguientes bloques
        respuesta_es = None


        # Comprobamos cu√°nto se tarda en determinar si se debe pasar o no a comprobar si usar RAG.
        # Para ello, iniciamos el cronometro de los dos m√≥dulos.
        with cronometro.medir("saludo_social_o_reexplicacion"):
            # En el caso de que la funci√≥n decisora (definida en este archivo) determine que la pregunta del
            # usuario es un f√≥rmula de cortes√≠a, entonces entramos aqu√≠.
            if self.es_saludo_social_llm(pregunta_traducida):
                # medimos cuanto tiempo se tarda en dar la respuesta y en generar su evaluacion en el m√≥dulo
                # social.
                with cronometro.medir("saludo_social"):
                    # Usamos la funci√≥n que se encuentra en social.py
                    # Esta funci√≥n nos devuelve la respuesta del modelo social (que es un texto).
                    respuesta_es = modulo_social(pregunta_traducida)
                    print("‚úÖ M√≥dulo social gener√≥ respuesta.")
                    # Iniciamos la evaluaci√≥n de la respuesta dada por el modelo.
                    try:
                        # Medimos el tiempo que se tarda en dar una evaluaci√≥n
                        with cronometro.medir("evaluacion_saludo"):
                            # Usamos la funci√≥n que se puede encontrar en evaluador_generativo.py
                            # Esto obtiene un JSON con los valores de la evaluacion
                            evaluacion_social = evaluar_saludo_social(pregunta_traducida, respuesta_es)
                            print("\nü§ù Evaluaci√≥n de saludo social:")
                            # Imprimimos el json
                            print(json.dumps(evaluacion_social, indent=2, ensure_ascii=False))

                            # Guardamos en un fichero la evaluci√≥n
                            os.makedirs("evaluacion/json", exist_ok=True)
                            with open("evaluacion/json/saludos.json", "a", encoding="utf-8") as f:
                                # Convertimos a un JSON v√°lido los valores
                                # N√≥tese, que ser√° un diccionario que contenga a otro diccionario porque evaluacion_social
                                # era un diccionario.
                                json.dump({
                                    "pregunta": pregunta_traducida,
                                    "respuesta": respuesta_es,
                                    "evaluacion": evaluacion_social
                                }, f, ensure_ascii=False)
                                # Escribimos una coma y un salto de l√≠nea para que se pueda escribir el siguiente elemento
                                f.write(",\n")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error al evaluar saludo: {e}")


            # Primero comprueba si hay algo en el historial de la conversaci√≥n.
            # Al haberse inicializado como [], si no hay nada esto es un objeto falsy que ser√° tomado como False
            # En caso de haber algo, comprueba si la funci√≥n definida en este fichero para buscar la reexplicacion
            # indica que esta es necesaria.
            # En caso de serlo, entramos en a dar la reexplicacion.
            elif self.historial_conversacion:
                necesita_reexplicacion = self.debe_reexplicar_llm(pregunta_traducida)
                if necesita_reexplicacion:
                    # Medimos cu√°nto tiempo tarda en dar una respuesta reexplicada y su evaluaci√≥n
                    with cronometro.medir("reexplicacion"):
                        print("üîÑ Se detect√≥ que el usuario no comprendi√≥, entrando en m√≥dulo de reexplicaci√≥n.")
                        # Vamos a reescribir el √∫ltimo mensaje del modelo por lo que tenemos que seleccionarlo
                        # Adem√°s, lo que vamos a reescribir es la respuesta por lo que tendremos que elegir esa clave
                        # de entre las que guardamos
                        ultimo_mensaje = self.historial_conversacion[-1]["respuesta"]
                        # Generamos la reexplicaci√≥n a partir del √∫ltimo mensaje del sistema usando la funci√≥n que 
                        # se encuentra en modulo_reexplicacion.py
                        respuesta_es = modulo_reexplicacion(ultimo_mensaje)
                        # Ahora pasamos a la evaluaci√≥n de este m√≥dulo.
                        # Nota: Esta solo se hace cuando el modelo da una respuesta, es decir, cuando hay historial
                        # previo.
                        try:
                            # Medimos cuanto tiempo tarda en generar la evaluacion
                            with cronometro.medir("evaluacion_reexplicacion"):
                                # Esta funci√≥n se encuentra en evaluador_generativo.py
                                evaluacion_reexplicacion = evaluar_reexplicacion(ultimo_mensaje, respuesta_es)
                                print("\nüîç Evaluaci√≥n de reexplicaci√≥n:")
                                print(json.dumps(evaluacion_reexplicacion, indent=2, ensure_ascii=False))

                                # Guardamos el resultado de las evaluaciones en un JSON
                                os.makedirs("evaluacion/json", exist_ok=True)
                                with open("evaluacion/json/reexplicaciones.json", "a", encoding="utf-8") as f:
                                    json.dump({
                                        "pregunta": pregunta_traducida,
                                        "original": ultimo_mensaje,
                                        "reexplicacion": respuesta_es,
                                        "evaluacion": evaluacion_reexplicacion
                                    }, f, ensure_ascii=False)
                                    f.write(",\n")
                        except Exception as e:
                            print(f"‚ö†Ô∏è Error al evaluar reexplicaci√≥n: {e}")
            # En caso de que no haya historial entramos aqu√≠
            # Entonces carga el modelo que comprueba la reexplicacion y si es necesaria da una respuesta por defecto
            # T√©ngase en cuenta, que tal y como lo hemos hecho se entra en el modelo que lo comprueba una sola vez
            else:
                necesita_reexplicacion = self.debe_reexplicar_llm(pregunta_traducida)
                if necesita_reexplicacion:
                    print("üîÑ Se detect√≥ que el usuario no comprendi√≥, entrando en m√≥dulo de reexplicaci√≥n.")
                    respuesta_es = "Disculpe, pero no hay ning√∫n mensaje previo que pueda explicarle. ¬øPuedo ayudarle en algo m√°s?"

        # Terminamos este bloque with.
        # Como no hemos puesto ning√∫n return no se sale de la funci√≥n.
        # Sin embargo, si ha entrado en alguno de los anteriores entonces respuesta_es tiene un valor y no
        # entra dentro del siguiente if si no que pasa directamente a lo siguiente que es la traducci√≥n.
        if respuesta_es is None:
            # Si no hay respuesta, iniciamos el crono para ver cu√°nto tiempo tardamos en resumir el historial
            with cronometro.medir("resumen_historial"):
                # Si hay historial de preguntas del usuario entramos a resumirlo
                if self.historial_preguntas_usuario:
                    # En caso de haber historial y no haber resumen, entramos en la funci√≥n que se encuentra en resumidor.py

                    # Preparamos la lista de tuplas (pregunta, respuesta)
                    lista_preg_resp = [(item["pregunta"], item["respuesta"]) for item in self.historial_conversacion]

                    # Llamamos a la funci√≥n obtener_contexto_previo del resumidor.py
                    # Para poder llamarla, hemos tenido que unir el contexto en respuesta y pregunta en una tupla
                    # la funci√≥n recibe como par√°metro un √∫nico texto.
                    # a su vez llama a la funci√≥n para obtener un resumen y nos devuelve el resumen del historial
                    historial_resumido = self.resumidor.obtener_contexto_previo(lista_preg_resp, self.historial_resumen)
                    # Nos guardamos nuestro nuevo historial
                    self.historial_resumen = historial_resumido

                else:
                    historial_resumido = ""
                    print("üìÑ No hay historial previo relevante, se evaluar√° la pregunta de forma aislada.")

            # Terminamos el with anterior
            # Ahora vamos a determinar si una pregunta es o no relevante para el RAG.
            with cronometro.medir("evaluacion_relevancia"):
                # Para hacer la comprobaci√≥n usamos una de las funciones definidas en este archivo.
                relevante = self.es_pregunta_relevante_llm(pregunta_traducida, historial_resumido)

            # En caso de ser relevante para RAG entramos en el siguiente m√≥dulo
            if relevante:
                print("‚úÖ La pregunta es relevante para RAG seg√∫n el historial.")
                # A√±adimos al historial de preguntas, la realizada por el usuario. 
                # Esto es porque de cara al historial solo vamos a tener en cuenta las preguntas que entren
                # en el RAG, el resto no se tendr√° en cuenta.
                self.historial_preguntas_usuario.append(pregunta_traducida)

                # Inicamos el cronometro para ver cu√°nto tiempo tarda en el m√≥dulo de RAG
                with cronometro.medir("chat_rag_buscar_respuesta"):
                    # Llamamos a la funci√≥n buscar respuesta que se encuentra dentro de la instancia creada en
                    # chatDoctorado.py
                    respuesta_es, chunks_rankeados = self.chat_rag.buscar_respuesta(
                        pregunta_traducida,
                        self.vectorstore,
                        top_k=self.top_k,
                        contexto_previo=historial_resumido
                    )

                # Si el modelo generativo devuelve la respuesta de que no tiene informaci√≥n, entramos en el if
                if "Lo siento, no tengo informaci√≥n" in respuesta_es:
                    # Indicamos que espera una aclaracion
                    self.esperando_aclaracion = True
                    # Guardamos la √∫ltima pregunta realizada
                    self.ultima_pregunta_original = pregunta_traducida

                    # Iniciamos el contador de tiempo
                    with cronometro.medir("generar_pregunta_clarificacion"):
                        # Nos vamos ahora a la funcion que se encuentra en clarificador.py
                        # Esto nos da un texto, en concreto una pregunta.
                        respuesta_es = generar_pregunta_clarificacion(pregunta_traducida)
                        # Iniciamos el cronometro para ver cuanto tarda en realizar la evaluacion
                        with cronometro.medir("evaluacion_clarificacion"):
                            try:
                                # Realizamos la evaluaci√≥n de este m√≥dulo usando la funci√≥n en evaluador_generativo.py
                                # Nos devolver√° un JSON aunque se produzcan fallos.
                                # Le damos la pregunta inicial del usuario, y la pregunta para realizar la expansi√≥n
                                # que hemos generado
                                evaluacion_clarificacion = evaluar_pregunta_clarificacion(pregunta_traducida, respuesta_es)
                                print("\nüß© Evaluaci√≥n de pregunta clarificadora:")
                                # Lo convertimos a objeto JSON
                                print(json.dumps(evaluacion_clarificacion, indent=2, ensure_ascii=False))

                                # Si no existe, creamos la carpeta
                                os.makedirs("evaluacion/json", exist_ok=True)
                                # Abrimos el archivo y guardamos la pregunta inicial, la respuesta generada por 
                                # el modelo y la evaluacion (la cual ser√° un json)
                                with open("evaluacion/json/clarificaciones.json", "a", encoding="utf-8") as f:
                                    json.dump({
                                        "pregunta": pregunta_traducida,
                                        "clarificacion": respuesta_es,
                                        "evaluacion": evaluacion_clarificacion
                                    }, f, ensure_ascii=False)
                                    # Escribimos al final un salto de l√≠nea.
                                    f.write(",\n")
                            except Exception as e:
                                print(f"‚ö†Ô∏è Error al evaluar pregunta clarificadora: {e}")

                    print("‚ùì Se requiere aclaraci√≥n del usuario.")
                # En caso de que lo que devuelva el modelo sea distinto, lo a√±adimos al historial como pregunta 
                # y respuesta
                else:
                    self.historial_conversacion.append({
                        "pregunta": pregunta_traducida,
                        "respuesta": respuesta_es
                    })
                    # Iniciamos el crono para medir el funcionamiento del rag
                    with cronometro.medir("evaluacion_rag"):
                        try:
                            # Obtener los chunks relevantes mediante el uso de la funcion en evaluador_rag.py
                            # Esto devuelve todos los chunks cuyo valor de similitud con la pregunta supera el dado
                            chunks_relevantes = obtener_todos_los_chunks_relevantes(
                                pregunta_traducida,
                                self.vectorstore,
                                umbral_similitud=0.75
                            )

                            # En este contexto, chunks_rag se refiere a los top_k documentos seleccionados, con su
                            # url y su texto
                            chunks_rag = chunks_relevantes[:self.top_k]

                            # Extraemos los chunks obtenidos en el reranking, pero solo seleccionamos la url y 
                            # el texto, la puntuaci√≥n aqu√≠ nos da igual.
                            chunks_reranking = [(url, ch) for url, ch, sc in chunks_rankeados]

                            # Le pasamos todos a la funci√≥n dentro de evaluador_rag.py
                            evaluacion = evaluar_respuesta_con_llm(
                                pregunta_traducida,
                                respuesta_es,
                                chunks_rag,
                                chunks_reranking,
                                chunks_relevantes
                            )
                            # Esto nos devuelve un JSON
                            # Nos aseguramos que chunk_omitidos exista y si no esta ponemos una lista vac√≠a
                            evaluacion["chunks_omitidos"] = evaluacion.get("chunks_omitidos", [])

                            # Mostramos por pantalla las evaluaciones
                            # Si no hay, devolvemos un diccionario vacio.
                            ev = evaluacion.get("evaluacion", {})
                            print("\nüìä Evaluaci√≥n RAG:")
                            print(f"   - Cobertura: {ev.get('cobertura')}")
                            print(f"   - Precisi√≥n: {ev.get('precisi√≥n')}")
                            print(f"   - Alucinaci√≥n: {ev.get('alucinacion')}")
                            print(f"   - Comentario: {ev.get('comentario')}\n")

                            # Creamos las carpetas si no exisetn
                            os.makedirs("evaluacion/json", exist_ok=True)
                            # Ruta del fichero donde vamos a guardar el json
                            ruta_eval_rag = "evaluacion/json/rag.json"

                            # Si la ruta del fichero existe, cargamos los datos con formato JSON
                            if os.path.exists(ruta_eval_rag):
                                with open(ruta_eval_rag, "r", encoding="utf-8") as f:
                                    datos_rag = json.load(f)
                            # En caso contrario, creamos una lista vacia
                            else:
                                datos_rag = []

                            # A√±adimos los nuevos datos de evaluacion
                            datos_rag.append(evaluacion)

                            # Guardamos todo en el fichero
                            with open(ruta_eval_rag, "w", encoding="utf-8") as f:
                                json.dump(datos_rag, f, ensure_ascii=False, indent=2)

                            print("‚úÖ Evaluaci√≥n RAG guardada en evaluacion/json/rag.json")

                        except Exception as e:
                            print(f"‚ö†Ô∏è Error al evaluar RAG: {e}")

            # En caso de no ser considerado relevante para RAG
            else:
                print("‚ùå Pregunta no relevante para RAG, usando modelo sin contexto.")
                # Iniciamos el cronometro para medir el tiempo que se tarda en el no rag
                with cronometro.medir("responder_no_rag"):
                    # Llamamos a la funcion en modelo_no_rag.py
                    respuesta_es = responder_no_rag(pregunta_traducida)
                    # Lo anterior nos devuelve siempre un texto.

                # Iniciamos el cron√≥metro para medir el tiempo que se tarda en evaluar esta parte de respuesta
                # general.
                with cronometro.medir("evaluacion_no_rag"):
                    try:
                        # Realizamos la evaluacion de la respuesta general a partir de la funci√≥n que podemos encontrar
                        # en evaluador_generativo.py
                        evaluacion_no_rag = evaluar_respuesta_no_rag(pregunta_traducida, respuesta_es)
                        print("\nüìã Evaluaci√≥n modelo sin RAG:")
                        # Esto nso devuelve un JSON con la evaluaci√≥n del modelo
                        print(json.dumps(evaluacion_no_rag, indent=2, ensure_ascii=False))

                        # Creamos el directorio correspondiente para guardar el archivo con el JSON obtenido
                        # de la evaluaci√≥n de la pregunta general
                        os.makedirs("evaluacion/json", exist_ok=True)
                        with open("evaluacion/json/no_rag.json", "a", encoding="utf-8") as f:
                            json.dump({
                                "pregunta": pregunta_traducida,
                                "respuesta": respuesta_es,
                                "evaluacion": evaluacion_no_rag
                            }, f, ensure_ascii=False)
                            # Escribimos un salto de l√≠nea al final
                            f.write(",\n")
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error al evaluar respuesta sin RAG: {e}")

        # Guardamos unas variables para determinar el idioma al que tenemos que traducir
        # En el caso de ser el idioma original el espa√±ol entonces no se traduce porque el idioma de destino
        # sera tambi√©n el espa√±ol
        # En caso contrario, primero se comprueba si hab√≠a mezcla de idiomas en cuyo caso el idioma de destino
        # ser√° el ingl√©s
        # En caso de que no hubiese mezcla entonces el idioma al que se traduce es el idioma original en el que 
        # ven√≠a el texto.
        if idioma_original == "espa√±ol":
            idioma_destino = "espa√±ol"
        elif mezcla:
            idioma_destino = "ingles"
        else:
            idioma_destino = idioma_original

        print(f"üåç Idioma original: {idioma_original} | Mezcla: {mezcla} | Traducci√≥n final: {idioma_destino}")

        # Ponemos un cronometro para medir el tiempo de traduccion de la respuesta final
        with cronometro.medir("traduccion_respuesta_final"):
            if idioma_destino != "espa√±ol":
                # En caso de no ser el idioma el espa√±ol, llamamos a la funci√≥n que se encuentra en traduccion_respuesta.py
                # Esta nos devuelve el texto traducido al idioma correspondiente
                respuesta_final = traducir_respuesta(respuesta_es, idioma_destino)
                print(f"‚úÖ Respuesta traducida del espa√±ol a: {idioma_destino}")
            else:
                # en caso contrario, la respuesta final ser√° la respuesta original ya que no habr√≠a que traducirla
                respuesta_final = respuesta_es
                print("‚úÖ La respuesta est√° en espa√±ol, no se traduce.")

        # Iniciamos la evaluaci√≥n para medir como de bien hace la traducci√≥n
        with cronometro.medir("evaluacion_traduccion"):
            # Solo en aquellos casos en los que el idioma no fuera espa√±ol es cuando tiene sentido ver como 
            # de bien o de mal hace la traducci√≥n.
            if idioma_original != "espa√±ol":
                try:
                    # Llamamos a la funcion en evaluador_traduccion.py, que nos devolver√° un JSON
                    resultado_eval = evaluar_traduccion(
                        texto=pregunta_usuario,
                        respuesta=respuesta_es,
                        src_lang=idioma_original,
                        tgt_lang="espa√±ol"
                    )

                    # Creamos la ruta del fichero si no existe
                    os.makedirs("evaluacion/json", exist_ok=True)
                    ruta_archivo = "evaluacion/json/traduccion.json"

                    # En caso de que el fichero exista, cargamos su contenido
                    if os.path.exists(ruta_archivo):
                        with open(ruta_archivo, "r", encoding="utf-8") as f:
                            datos = json.load(f)
                    else:
                        # En caso contrario creamos una lista vacia
                        datos = []

                    # A√±adimos los resultados al fichero
                    datos.append(resultado_eval)

                    # Guardamos el fichero de datos
                    with open(ruta_archivo, "w", encoding="utf-8") as f:
                        json.dump(datos, f, ensure_ascii=False, indent=2)

                    print("‚úÖ Evaluaci√≥n de traducci√≥n guardada en evaluacion/json/traduccion.json")

                except Exception as e:
                    print(f"‚ùå Error al evaluar o guardar traducci√≥n: {e}")

        cronometro.guardar_json()

        return {
            "respuesta": respuesta_final,
            "necesita_aclaracion": self.esperando_aclaracion
        }

    def reiniciar_contexto(self):
        """
        Esta funci√≥n sirve para reiniciar la conversaci√≥n.
        
        """
        # Eliminamos los historiales de conversaci√≥n
        self.historial_conversacion.clear()
        self.historial_preguntas_usuario.clear()
        # Ponemos los valores predefinidos
        self.esperando_aclaracion = False
        self.ultima_pregunta_original = None
        print("üîÑ Contexto reiniciado. Puedes empezar una nueva consulta.")

    def chat_interactivo(self):
        """
        Llamamos al chat con el modelo.
        
        """
        # Mensaje inicial
        print(
            "üí¨ Chat con AsistenteUS (escribe 'salir', 'exit', 'quit' para salir o 'reiniciar' para empezar de nuevo): \n"
            "Soy el asistente virtual de la Universidad de Sevilla sobre temas de doctorados. \n ¬øEn qu√© puedo ayudarte?"
        )

        # Mientras que el usuario no se salga
        while True:
            # Recogemos el input del usuario
            user_input = input("\nüë§ T√∫: ").strip()
            # Si dice de salir entonces reiniciamos el contexto y salimos de la conversaci√≥n, para dejarlo
            # limpio para la pr√≥xima.
            if user_input.lower() in ["salir", "exit", "quit"]:
                self.reiniciar_contexto()
                print("üëã Saliendo del chat...")
                break
            # Si el usuario indica que lo quiere reiniciar, llamamos a la funci√≥n para reiniciar el contexto
            if user_input.lower() == "reiniciar":
                self.reiniciar_contexto()
                continue
            # Cualquier otra cosa, llamamos a la funci√≥n responder definida anteriormente
            resultado = self.responder(user_input)
            # Devolvemos la respuesta guardada en el json de resultado al usuario
            print(f"\nüß† AsistenteUS: {resultado['respuesta']}")


if __name__ == "__main__":
    agente = AgenteDecisor(top_k=5)
    agente.chat_interactivo()